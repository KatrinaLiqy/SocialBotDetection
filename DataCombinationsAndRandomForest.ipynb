{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KatrinaLiqy/SocialBotDetection/blob/main/DataCombinationsAndRandomForest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCt_6UQcrZHs",
        "outputId": "d15124a5-1e4d-4525-f878-547b62d8a069"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "dataset_folder = 'data_csv/final'\n",
        "\n",
        "#this directory contains the csv with all the models' metrics, and all tables generated in this notebook will go here\n",
        "results_path = \"./results/5_final_results/\"\n",
        "models_path = \"./models/5_final_results/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ba2CieImhqv"
      },
      "source": [
        "### 119 Datasets Combinations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bQHO_Vd1jbE7"
      },
      "outputs": [],
      "source": [
        "# Seven training datasets\n",
        "varol = pd.read_csv(dataset_folder + '/varol-17.csv')\n",
        "cresci_17 = pd.read_csv(dataset_folder + '/cresci-17.csv')\n",
        "pronbots = pd.read_csv(dataset_folder + '/pronbots-2019.csv')\n",
        "celebrity = pd.read_csv(dataset_folder + '/celebrity-2019.csv')\n",
        "vendor = pd.read_csv(dataset_folder + '/vendor-purchased-2019.csv')\n",
        "botometer = pd.read_csv(dataset_folder + '/botometer-feedback-2019.csv')\n",
        "political = pd.read_csv(dataset_folder + '/political-bots-2019.csv')\n",
        "# Testin\n",
        "botwiki = pd.read_csv(dataset_folder + '/botwiki-verified.csv')\n",
        "midterm = pd.read_csv(dataset_folder + '/midterm-2018.csv')\n",
        "gilani = pd.read_csv(dataset_folder + '/gilani-2017.csv')\n",
        "c_rtbust = pd.read_csv(dataset_folder + '/cresci-rtbust-2019.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DvRSilLmMdYH"
      },
      "outputs": [],
      "source": [
        "# 0. varol-icwsm - B733 H1495\n",
        "# 1. cresci-17 - B7049 H2764\n",
        "# 2. pronbots - Only Bots 17882\n",
        "# 3. celebrity - Only Humans 5918\n",
        "# 4. vendor-purchased - Only Bots 1087\n",
        "# 5. botometer-feedback - B139 H380\n",
        "# 6. political-bots - Only Bots 62\n",
        "\n",
        "dataset_list = [varol, cresci_17, pronbots, celebrity, vendor, botometer, political] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGGF8BQ_ROqG",
        "outputId": "a4d375b8-77a3-4965-ff1f-969ac1a07f79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "119\n"
          ]
        }
      ],
      "source": [
        "all_combinations = []\n",
        "for i in range(1,128):\n",
        "  combination = bin(i).replace('0b','')\n",
        "  combination = combination.zfill(7)\n",
        "  all_combinations.append(combination)\n",
        "\n",
        "# Manually remove the 8 combinations that are only bots or humans\n",
        "all_combinations.remove('0010101')\n",
        "all_combinations.remove('0010100')\n",
        "all_combinations.remove('0010001')\n",
        "all_combinations.remove('0000101')\n",
        "all_combinations.remove('0000001')\n",
        "all_combinations.remove('0000100')\n",
        "all_combinations.remove('0010000')\n",
        "all_combinations.remove('0001000')\n",
        "\n",
        "print(len(all_combinations))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWsYpKHOhtoJ",
        "outputId": "59658695-c359-4c9c-ea61-60d535324efe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "119\n"
          ]
        }
      ],
      "source": [
        "all_combinations_data = [] # for storing all combined data\n",
        "for combination in all_combinations:\n",
        "  data = pd.DataFrame()\n",
        "  for j in range(7):\n",
        "    if combination[j] == '1':\n",
        "      data = pd.concat([data,dataset_list[j]],axis=0,ignore_index=True)\n",
        "  all_combinations_data.append(data)\n",
        "  \n",
        "print(len(all_combinations_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9RlBhcgmJht"
      },
      "source": [
        "### Functions for Training & Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pLMGmwFWuL89"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import metrics\n",
        "from joblib import dump, load\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import csv\n",
        "from sklearn.model_selection import StratifiedShuffleSplit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "M8VEHRAcyYlO"
      },
      "outputs": [],
      "source": [
        "# Four testing sets features and labels splits\n",
        "\n",
        "column_number = len(botwiki.columns)  # all data frame should have 21 columns. \n",
        "X_test_botwiki = botwiki.iloc[:, 0:column_number - 1]\n",
        "X_test_midterm = midterm.iloc[:, 0:column_number - 1]\n",
        "X_test_gilani = gilani.iloc[:, 0:column_number - 1]\n",
        "X_test_rtbust = c_rtbust.iloc[:, 0:column_number - 1]\n",
        "\n",
        "y_test_botwiki = botwiki.iloc[:, column_number - 1]\n",
        "y_test_midterm = midterm.iloc[:, column_number - 1]\n",
        "y_test_gilani = gilani.iloc[:, column_number - 1]\n",
        "y_test_rtbust = c_rtbust.iloc[:, column_number - 1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "CDM9Iov-OBao"
      },
      "outputs": [],
      "source": [
        "def train(all_combinations_data, model_path, random_state_value):\n",
        "  cv_auc_list = []\n",
        "  for i, data in enumerate(all_combinations_data):\n",
        "    X_train = data.iloc[:, 0:20]\n",
        "    y_train = data.iloc[:, 20]\n",
        "  \n",
        "    clf = RandomForestClassifier(n_estimators=100, class_weight=\"balanced\", random_state=random_state_value)\n",
        "    print(i)\n",
        "    clf.fit(X_train, y_train)\n",
        "    dump(clf, model_path + all_combinations[i]+'.joblib')\n",
        "  \n",
        "    # Do cross validation on training datasets\n",
        "    clf2 = RandomForestClassifier(n_estimators=100, class_weight=\"balanced\", random_state=random_state_value)\n",
        "    ss = StratifiedShuffleSplit(n_splits=5, test_size=0.2)\n",
        "    cv_auc_scores = cross_val_score(clf2, X_train, y_train, cv=ss, scoring='roc_auc') \n",
        "    cv_auc_list.append(sum(cv_auc_scores)/len(cv_auc_scores))\n",
        "  return cv_auc_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ROcO9JHBPisb"
      },
      "outputs": [],
      "source": [
        "def evaluation(cv_auc_list, model_path, result_path, results_file):\n",
        "  #get the test streaming/botometer correlation data\n",
        "  stream_df = pd.read_csv(dataset_folder + \"/stream_users.csv\")\n",
        "  stream_test = stream_df.iloc[:, 1:]\n",
        "\n",
        "  #get botometer predictions\n",
        "  boto_df = pd.read_csv(dataset_folder + \"/stream_account_scores.csv\").drop(\"cap\", axis=1)\n",
        "\n",
        "  with open(result_path + results_file, \"w\") as csvfile:\n",
        "    csv_writer = csv.writer(csvfile)\n",
        "\n",
        "    csv_writer.writerow([\"name\", \"botwiki-verified\", \"midterm-18\", \"gilani-17\", \"cresci-rtbust\", \"5-fold_cross-validation\", \"spearman_r\"])\n",
        "\n",
        "    for i, binary_str in enumerate(all_combinations):\n",
        "      model = load(model_path + binary_str +'.joblib')\n",
        "\n",
        "    \n",
        "      #make predictions on the streaming twitter account data\n",
        "      prediction = pd.DataFrame(model.predict_proba(stream_test), columns = [\"human_prob\", \"bot_prob\"]).drop(\"human_prob\", axis=1)\n",
        "      name_prediction = pd.concat([stream_df[\"screen_name\"], prediction], axis=1)\n",
        "      combined_data = boto_df.merge(name_prediction, on=\"screen_name\")\n",
        "    \n",
        "      spearman_r = combined_data.corr(method=\"spearman\")[\"overall\"][\"bot_prob\"]\n",
        "      \n",
        "      #do cross domain validation on new datasets\n",
        "      botwiki_roc = roc_auc_score(y_test_botwiki, model.predict_proba(X_test_botwiki)[:, -1])\n",
        "      midterm_roc = roc_auc_score(y_test_midterm, model.predict_proba(X_test_midterm)[:, -1])\n",
        "      gilani_roc = roc_auc_score(y_test_gilani, model.predict_proba(X_test_gilani)[:, -1])\n",
        "      rtbust_roc = roc_auc_score(y_test_rtbust, model.predict_proba(X_test_rtbust)[:, -1])\n",
        "\n",
        "      cross_validation_score = cv_auc_list[i]\n",
        "      csv_writer.writerow([binary_str, botwiki_roc, midterm_roc, gilani_roc, rtbust_roc, cross_validation_score, spearman_r])\n",
        "\n",
        "\n",
        "      print(f\"Evaluated {binary_str}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "87mgYZQedMlP"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "SRZ7GejBRn4Y"
      },
      "outputs": [],
      "source": [
        "def save_params(model_path, result_path, all_combinations):\n",
        "  binary_str = all_combinations[0]\n",
        "  model = load(model_path + binary_str +'.joblib')\n",
        "\n",
        "  params = model.get_params()\n",
        "  print(params)\n",
        "\n",
        "  json_file = os.path.join(result_path, \"hyperparameters.json\")\n",
        "\n",
        "  with open(json_file, \"w\") as outfile:\n",
        "    outfile.write(json.dumps(params))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RPn5PqMngzo"
      },
      "source": [
        "### Training and Testing Function Calls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_b3UmpXddQd",
        "outputId": "7989ed46-3ed4-41bb-da7c-19234b81be48"
      },
      "outputs": [],
      "source": [
        "# trained the model and evaluated with random_state = 0,1,2,3,4 into results_csv\n",
        "for i in range(1,5,1):\n",
        "  cv_list = train(all_combinations_data, models_path, random_state_value=i)\n",
        "  evaluation(cv_list, models_path, results_path, \"results_\" + str(i) + \".csv\")\n",
        "  save_params(models_path, results_path, all_combinations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nan2sya-d7E6"
      },
      "outputs": [],
      "source": [
        "result1 = pd.read_csv(results_path + \"results_0.csv\")\n",
        "result2 = pd.read_csv(results_path + \"results_1.csv\")\n",
        "result3 = pd.read_csv(results_path + \"results_2.csv\")\n",
        "result4 = pd.read_csv(results_path + \"results_3.csv\")\n",
        "result5 = pd.read_csv(results_path + \"results_4.csv\")\n",
        "\n",
        "df_concat = pd.concat((result1,result2,result3))\n",
        "df_combined = df_concat.groupby(df_concat.index).agg(['mean','count','std'])\n",
        "\n",
        "df_combined_mean = df_concat.groupby(df_concat.index).mean()\n",
        "df_combined_mean.to_csv(results_path+\"mean.csv\", index=False)\n",
        "\n",
        "df_combined_std = df_concat.groupby(df_concat.index).std()\n",
        "df_combined_std = df_combined_std.drop(columns=\"name\")\n",
        "df_combined_std = df_combined_mean['name'].to_frame().join(df_combined_std) # add the name column to the left\n",
        "df_combined_std.to_csv(results_path+\"std.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRmruknxSN3X"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "provenance": []
    },
    "interpreter": {
      "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    },
    "kernelspec": {
      "display_name": "Python 3.9.14 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
