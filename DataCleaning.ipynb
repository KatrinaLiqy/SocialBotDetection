{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uPD3QeD0ywX"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnFhwk1204NS",
        "outputId": "8f99cb9f-06d7-4df4-ad88-248e8a757e05"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "from dateutil.parser import parse\n",
        "import sys\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "# drive.mount('/content/drive', force_remount=True)\n",
        "# GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'botsdata' #change this to whatever folder your code is in\n",
        "# GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
        "\n",
        "# #print(os.listdir(GOOGLE_DRIVE_PATH))\n",
        "\n",
        "# sys.path.append(GOOGLE_DRIVE_PATH)\n",
        "\n",
        "original_datasets_path = \"./original_datasets/\"\n",
        "final_datasets_path = \"./data_csv/final/\"\n",
        "intermediate_path = \"./data_csv/intermediate/\"\n",
        "\n",
        "train_data = ['varol-17','cresci-17','pronbots-2019','celebrity-2019','vendor-purchased-2019','botometer-feedback-2019','political-bots-2019']\n",
        "part_one_files = ['cresci-17','midterm-2018','varol-17']\n",
        "part_two_files = ['botometer-feedback-2019', 'botwiki-2019','celebrity-2019', 'cresci-rtbust-2019', 'gilani-2017','political-bots-2019','pronbots-2019','vendor-purchased-2019','verified-2019']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vK03daQVUfV_"
      },
      "source": [
        "Zeroth, human screen name collection part 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "006iG3e-6YVY"
      },
      "outputs": [],
      "source": [
        "#initialize screen_names dict\n",
        "screen_names = {\"contains_dataset\":[]}\n",
        "\n",
        "#handle cresci data:\n",
        "screen_names[\"contains_dataset\"].append(\"cresci-17\")\n",
        "#cresci's only humans are located in the genuine-accounts.csv directory\n",
        "cresci_genuine_path = f\"{original_datasets_path}cresci-17/genuine_accounts.csv/users.csv\"\n",
        "cresci_df = pd.read_csv(cresci_genuine_path)\n",
        "for screen_name in cresci_df[\"screen_name\"]:\n",
        "    if screen_name not in screen_names:\n",
        "        screen_names[screen_name] = 1\n",
        "    else:\n",
        "        screen_names[screen_name] += 1\n",
        "\n",
        "#handle varol data - from the intermediate csv:\n",
        "screen_names[\"contains_dataset\"].append(\"varol-17\")\n",
        "varol_intermediate_path = intermediate_path + \"varol-17.csv\"\n",
        "varol_df = pd.read_csv(varol_intermediate_path)\n",
        "for index, row in varol_df.iterrows():\n",
        "    if row[\"is_bot\"] == 0:\n",
        "        screen_name = row[\"screen_name\"]\n",
        "        if screen_name not in screen_names:\n",
        "            screen_names[screen_name] = 1\n",
        "        else:\n",
        "            screen_names[screen_name] += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F018XIyt8XIj"
      },
      "source": [
        "First, human screen name collection part 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "OC5oKJGO06AB"
      },
      "outputs": [],
      "source": [
        "for filename in part_two_files:\n",
        "  tsvname = os.path.join(original_datasets_path, filename, f\"{filename}.tsv\")\n",
        "  jsonname = os.path.join(original_datasets_path, filename, f\"{filename}_tweets.json\")\n",
        "\n",
        "  bot_or_human_df = pd.read_csv(tsvname, sep='\\t',names = ['id','type'])\n",
        "  f = open(jsonname)\n",
        "  data = json.load(f)\n",
        "\n",
        "  collecting = False\n",
        "  if (filename not in screen_names[\"contains_dataset\"]) and (filename in train_data):\n",
        "    collecting = True\n",
        "    screen_names[\"contains_dataset\"].append(filename)\n",
        "\n",
        "  for line in data:\n",
        "    screen_name = line['user']['screen_name']\n",
        "    user = int(line['user']['id_str'])\n",
        "    temp_df = bot_or_human_df.loc[bot_or_human_df['id'] == user, 'type']\n",
        "    if temp_df.size >= 1:\n",
        "      if (temp_df.iloc[0] == 'human') and collecting:\n",
        "          if screen_name not in screen_names.keys():\n",
        "            screen_names[screen_name] = 1\n",
        "          else:\n",
        "            screen_names[screen_name] += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXSXfoTa9llJ"
      },
      "source": [
        "Second, screen name bigram probability calculations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "fD5HAkLc9r9q"
      },
      "outputs": [],
      "source": [
        "del screen_names[\"contains_dataset\"]\n",
        "\n",
        "all_names = screen_names.keys()\n",
        "\n",
        "#credit to ....\n",
        "def get_char_ngrams(string, n):\n",
        "    \"\"\"\n",
        "    This function takes a string and a positive integer n as input\n",
        "\n",
        "    It returns all character n-grams from the string of size n\n",
        "    They are stored in a list\n",
        "    \"\"\"\n",
        "    char_ngrams = []\n",
        "    for i in range(len(string) - n + 1):\n",
        "        char_ngrams.append(string[i:i+n])\n",
        "    return char_ngrams\n",
        "\n",
        "def _normalize_helper(dictionary):\n",
        "    \"\"\"\n",
        "    Normalize a single dictionary\n",
        "    \"\"\"\n",
        "    new_dict = {}\n",
        "    total = sum(dictionary.values())\n",
        "    for k, v in dictionary.items():\n",
        "        new_dict[k] = v / total\n",
        "    return new_dict\n",
        "\n",
        "def normalize(dictionary):\n",
        "    \"\"\"\n",
        "    This function takes dictionaries in two formats as input:\n",
        "    1. A dictionary where the keys are strings and the values are numbers\n",
        "                        OR\n",
        "    2. A dictionary of dictionaries. The keys of the inner dictionaries should be strings, and the values should be numbers.\n",
        "\n",
        "    It returns a new dictionary that is normalized such that the values add up to 1, representing a probability distribution. If the input is a dictionary of dictionaries, it normalizes all of the sub-dictionaries.\n",
        "    \"\"\"\n",
        "    if type(list(dictionary.values())[0]) == int:\n",
        "        return _normalize_helper(dictionary)\n",
        "\n",
        "    new_dict = {}\n",
        "    for k, v_dict in dictionary.items():\n",
        "        new_dict[k] = _normalize_helper(v_dict)\n",
        "    return new_dict\n",
        "\n",
        "chars = []\n",
        "for c in range(ord('A'), ord('Z')+1):\n",
        "  chars.append(chr(c))\n",
        "for c in range(ord('a'), ord('z')+1):\n",
        "  chars.append(chr(c))\n",
        "for c in range(0,10):\n",
        "  chars.append(str(c))\n",
        "chars.append('_')\n",
        "\n",
        "unigram = {}\n",
        "conditional = {}\n",
        "for name in all_names:\n",
        "  bigrams = get_char_ngrams(name, 2)\n",
        "  for bigram in bigrams:\n",
        "    if bigram[0] in unigram.keys():\n",
        "      unigram[bigram[0]] += 1\n",
        "    else:\n",
        "      unigram[bigram[0]] = 1\n",
        "\n",
        "    if bigram[0] in conditional.keys():\n",
        "      if bigram[1] in conditional[bigram[0]].keys():\n",
        "        conditional[bigram[0]][bigram[1]] += 1\n",
        "      else:\n",
        "        conditional[bigram[0]][bigram[1]] = 1\n",
        "    else:\n",
        "      conditional[bigram[0]] = {}\n",
        "      conditional[bigram[0]][bigram[1]] = 1\n",
        "\n",
        "\n",
        "# add 1 smoothing\n",
        "\n",
        "# for conditional\n",
        "for c1 in chars:\n",
        "  if c1 in conditional.keys():\n",
        "    for c2 in chars:\n",
        "      if c2 in conditional[c1].keys():\n",
        "        conditional[c1][c2] += 1\n",
        "      else:\n",
        "        conditional[c1][c2] = 1\n",
        "  else:\n",
        "    conditional[c1] = {}\n",
        "    for c2 in chars:\n",
        "      conditional[c1][c2] = 1\n",
        "\n",
        "# for unigram\n",
        "for c in chars:\n",
        "  if c in unigram.keys():\n",
        "    unigram[c] += 2*len(chars)\n",
        "  else:\n",
        "    unigram[c] = 2*len(chars)\n",
        "\n",
        "\n",
        "conditional = normalize(conditional)\n",
        "unigram = normalize(unigram)\n",
        "\n",
        "\n",
        "bigram_probs = {}\n",
        "for c1 in chars:\n",
        "  bigram_probs[c1] = {}\n",
        "  for c2 in chars:\n",
        "    bigram_probs[c1][c2] = conditional[c1][c2]*unigram[c1]\n",
        "\n",
        "\n",
        "def likelihood(screen_name):\n",
        "  likelihood = 1\n",
        "  bigram_name = get_char_ngrams(screen_name, 2)\n",
        "  for bigram in bigram_name:\n",
        "    likelihood *= bigram_probs[bigram[0]][bigram[1]]\n",
        "  likelihood = pow(likelihood,1/len(bigram_name))\n",
        "  return likelihood"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kf565IZ8q3N"
      },
      "source": [
        "Third, part_one_files data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "n_WQ_WbWPCM7"
      },
      "outputs": [],
      "source": [
        "columns_order = [\"statuses_count\", \"followers_count\", \"friends_count\", \"favourites_count\", \"listed_count\", \"default_profile\", \"profile_use_background_image\", \"verified\", \"tweet_freq\", \"followers_growth_rate\", \"friends_growth_rate\", \"favourites_growth_rate\", \"listed_growth_rate\", \"followers_friends_ratio\", \"screen_name_length\", \"num_digits_in_screen_name\", \"name_length\", \"num_digits_in_name\", \"description_length\", \"screen_name_likelihood\", \"is_bot\"]\n",
        "def derive_features(df):\n",
        "    #get derived features from the \n",
        "    df['tweet_freq'] = df.apply(lambda row: row['statuses_count'] / row['age_hr'], axis=1)\n",
        "    df['followers_growth_rate'] = df.apply(lambda row: row['followers_count'] / row['age_hr'], axis=1)\n",
        "    df['friends_growth_rate'] = df.apply(lambda row: row['friends_count'] / row['age_hr'], axis=1)\n",
        "    df['favourites_growth_rate'] = df.apply(lambda row: row['favourites_count'] / row['age_hr'], axis=1)\n",
        "    df['listed_growth_rate'] = df.apply(lambda row: row['listed_count'] / row['age_hr'], axis=1)\n",
        "    df['followers_friends_ratio'] = df.apply(lambda row: row['followers_count'] / max(1, row['friends_count']), axis=1)\n",
        "    df['screen_name_length'] = df.apply(lambda row: len(row['screen_name']), axis=1)\n",
        "    df['num_digits_in_screen_name'] = df.apply(lambda row: sum(c.isdigit() for c in row['screen_name']), axis=1)\n",
        "    df['name_length'] = df.apply(lambda row: len(str(row['name'])), axis=1)\n",
        "    df['num_digits_in_name'] = df.apply(lambda row: sum(c.isdigit() for c in row['name']), axis=1)\n",
        "    df['description_length'] = df.apply(lambda row: len(row['description']), axis=1)\n",
        "    df['screen_name_likelihood'] = df.apply(lambda row: likelihood(row['screen_name']), axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "#clean cresci\n",
        "cresci_dir = os.path.join(original_datasets_path, \"cresci-17\")\n",
        "output_path = os.path.join(final_datasets_path, \"cresci-17.csv\")\n",
        "\n",
        "assert not os.path.exists(output_path), f\"{output_path} already exists\"\n",
        "\n",
        "columns = [\"id\", \"timestamp\", \"crawled_at\", \"name\", \"screen_name\", \"description\", \"statuses_count\", \"followers_count\", \"friends_count\", \"favourites_count\", \"listed_count\", \"default_profile\", \"profile_use_background_image\", \"verified\"]\n",
        "def get_age_hr(row):\n",
        "    age = parse(str(row['crawled_at'])) -  parse(str(row['timestamp']))\n",
        "\n",
        "    return age.days * 24 + age.seconds / 3600\n",
        "\n",
        "for dir in  os.listdir(cresci_dir):\n",
        "    input_csv = os.path.join(cresci_dir, dir, \"users.csv\")\n",
        "\n",
        "    df = pd.read_csv(input_csv)\n",
        "    df = df[columns]\n",
        "\n",
        "\n",
        "    df['age_hr'] = df.apply(func = get_age_hr, axis=1)\n",
        "    df['name'] = df['name'].astype(str)\n",
        "    df['screen_name'] = df['screen_name'].astype(str)\n",
        "    df['description'] = df['description'].astype(str)\n",
        "    df['profile_use_background_image'] = df.apply(lambda row: row['profile_use_background_image'] == 1, axis=1)\n",
        "    df['default_profile'] = df.apply(lambda row: row['default_profile'] == 1, axis=1)\n",
        "    df['verified'] = df.apply(lambda row: row['verified'] == 1, axis=1)\n",
        "\n",
        "    derive_features(df)\n",
        "\n",
        "    #only genuine is not a bot\n",
        "    if \"genuine\" in input_csv:\n",
        "        df[\"is_bot\"] = 0\n",
        "    else:\n",
        "        df[\"is_bot\"] = 1\n",
        "\n",
        "    df = df[columns_order]\n",
        "    df.to_csv(output_path, mode='a', header=not os.path.exists(output_path), index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "#clean midterm-18\n",
        "input_json = os.path.join(original_datasets_path, \"midterm-18/midterm-2018_processed_user_objects.json\")\n",
        "input_labels = os.path.join(original_datasets_path, \"midterm-18/midterm-2018.tsv\")\n",
        "output_path = os.path.join(final_datasets_path, \"midterm-2018.csv\")\n",
        "\n",
        "assert not os.path.exists(output_path), f\"{output_path} already exists\"\n",
        "\n",
        "df = pd.read_json(input_json)\n",
        "\n",
        "def get_age_hr(row):\n",
        "    age = (parse(str(row['probe_timestamp'])) -  parse(str(row['user_created_at'])))\n",
        "\n",
        "    return age.days * 24 + age.seconds / 3600\n",
        "\n",
        "df['age_hr'] = df.apply(func = get_age_hr, axis=1)\n",
        "\n",
        "df['name'] = df['name'].astype(str)\n",
        "df['screen_name'] = df['screen_name'].astype(str)\n",
        "df['description'] = df['description'].astype(str)\n",
        "\n",
        "derive_features(df)\n",
        "\n",
        "labels = pd.read_csv(input_labels, delim_whitespace=True, names=[\"user_id\", \"is_bot\"])\n",
        "\n",
        "labels[\"is_bot\"] = labels.apply(lambda row: 1 if row[\"is_bot\"] == \"bot\" else 0, axis=1)\n",
        "\n",
        "df = df.merge(labels, on=\"user_id\")\n",
        "\n",
        "df = df[columns_order]\n",
        "\n",
        "df.to_csv(output_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "#derive the features of stream_data and varol from their intermediate csvs\n",
        "intermediate_csvs = [os.path.join(intermediate_path, \"varol-17.csv\"), os.path.join(intermediate_path, \"stream_users.csv\")]\n",
        "output_paths = [os.path.join(final_datasets_path, \"varol-17.csv\"), os.path.join(final_datasets_path, \"stream_users.csv\")]\n",
        "\n",
        "for input_path, output_path in zip(intermediate_csvs, output_paths):\n",
        "    df = pd.read_csv(input_path)\n",
        "    \n",
        "    def get_age_hr(row):\n",
        "        age = (parse(str(row['sampled_at'])[:-6]) -  parse(str(row['created_at'])[:-6]))\n",
        "\n",
        "        return age.days * 24 + age.seconds / 3600\n",
        "\n",
        "    df['age_hr'] = df.apply(func = get_age_hr, axis=1)\n",
        "\n",
        "    df['name'] = df['name'].astype(str)\n",
        "    df['screen_name'] = df['screen_name'].astype(str)\n",
        "    df['description'] = df['description'].astype(str)\n",
        "\n",
        "    df['profile_use_background_image'] = df.apply(lambda row: row['banner_url'] != 'N/A', axis=1)\n",
        "\n",
        "    derive_features(df)\n",
        "\n",
        "    if \"is_bot\" in list(df.columns):\n",
        "        df = df[columns_order]\n",
        "    else:\n",
        "        df = df[columns_order[:-1]]\n",
        "        \n",
        "    df.to_csv(output_path, index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UX3O8wl48y31"
      },
      "source": [
        "Fourth, part_two_files data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "PaC4NVd4507X"
      },
      "outputs": [],
      "source": [
        "for filename in part_two_files:\n",
        "  tsvname = os.path.join(original_datasets_path, filename, f\"{filename}.tsv\")\n",
        "  jsonname = os.path.join(original_datasets_path, filename, f\"{filename}_tweets.json\")\n",
        "  outname =  os.path.join(final_datasets_path, filename + '.csv')\n",
        "\n",
        "  bot_or_human_df = pd.read_csv(tsvname, sep='\\t',names = ['id','type'])\n",
        "  f = open(jsonname)\n",
        "  data = json.load(f)\n",
        "\n",
        "  arr = []\n",
        "  for line in data:\n",
        "    row = []\n",
        "    user = int(line['user']['id_str'])\n",
        "    # row.append(line['user']['id_str'])\n",
        "    row.append(line['user']['statuses_count'])\n",
        "    row.append(line['user']['followers_count'])\n",
        "    row.append(line['user']['friends_count'])\n",
        "    row.append(line['user']['favourites_count'])\n",
        "    row.append(line['user']['listed_count'])\n",
        "    row.append(line['user']['default_profile'])\n",
        "    row.append(line['user']['profile_use_background_image'])\n",
        "    row.append(line['user']['verified'])\n",
        "\n",
        "\n",
        "    dt_born = parse(line['user']['created_at'])\n",
        "    dt_curr = parse(line['created_at'])\n",
        "    age = dt_curr - dt_born\n",
        "    user_age = age.days*24 + age.seconds/3600\n",
        "\n",
        "    row.append(line['user']['statuses_count'] / user_age)\n",
        "    row.append(line['user']['followers_count'] / user_age)\n",
        "    row.append(line['user']['friends_count'] / user_age)\n",
        "    row.append(line['user']['favourites_count'] / user_age)\n",
        "    row.append(line['user']['listed_count'] / user_age)\n",
        "    row.append(line['user']['followers_count'] / max(1,line['user']['friends_count']))\n",
        "\n",
        "    screen_name = line['user']['screen_name']\n",
        "    name = line['user']['name']\n",
        "\n",
        "    row.append(len(screen_name))\n",
        "    row.append(sum(c.isdigit() for c in screen_name))\n",
        "    row.append(len(name))\n",
        "    row.append(sum(c.isdigit() for c in name))\n",
        "    row.append(len(line['user']['description']))\n",
        "    row.append(str(likelihood(screen_name)))\n",
        "\n",
        "\n",
        "\n",
        "    df2 = bot_or_human_df.loc[bot_or_human_df['id'] == user, 'type']\n",
        "    if df2.size >= 1:\n",
        "      if (df2.iloc[0] == 'human'):\n",
        "        row.append('0')\n",
        "        arr.append(row)\n",
        "        \n",
        "      elif (df2.iloc[0] == 'bot'):\n",
        "        row.append('1')\n",
        "        arr.append(row)\n",
        "\n",
        "  f.close()\n",
        "\n",
        "\n",
        "  with open(outname, 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile, delimiter=',',\n",
        "                            quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
        "    writer.writerow(['statuses_count','followers_count','friends_count','favourites_count','listed_count','default_profile','profile_use_background_image','verified','tweet_freq','followers_growth_rate','friends_growth_rate','favourites_growth_rate','listed_growth_rate', 'followers_friends_ratio', 'screen_name_length','num_digits_in_screen_name', 'name_length','num_digits_in_name','description_length','screen_name_likelihood', 'is_bot'])\n",
        "    for row in arr:\n",
        "      writer.writerow(row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "#merge botwiki and verified\n",
        "botwiki_df = pd.read_csv(os.path.join(final_datasets_path, \"botwiki-2019.csv\"))\n",
        "verified_df = pd.read_csv(os.path.join(final_datasets_path, \"verified-2019.csv\"))\n",
        "\n",
        "pd.concat([botwiki_df, verified_df], ignore_index=True).to_csv(os.path.join(final_datasets_path, \"botwiki-verified.csv\"), index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
