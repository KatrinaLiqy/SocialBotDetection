{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uPD3QeD0ywX"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from dateutil.parser import parse\n",
        "import sys\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'botsdata' #change this to whatever folder your code is in\n",
        "GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
        "\n",
        "#print(os.listdir(GOOGLE_DRIVE_PATH))\n",
        "\n",
        "sys.path.append(GOOGLE_DRIVE_PATH)\n",
        "\n",
        "train_data = ['varol-17','cresci-17','pronbots-2019','celebrity-2019','vendor-purchased-2019','botometer-feedback-2019','political-bots-2019']\n",
        "part_one_files = ['botwiki-verified','cresci-17','cresci-rtbust-2019','midterm-2018','varol-17']\n",
        "part_two_files = ['botometer-feedback-2019', 'botwiki-2019','celebrity-2019','gilani-2017','political-bots-2019','pronbots-2019','vendor-purchased-2019','verified-2019']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnFhwk1204NS",
        "outputId": "8f99cb9f-06d7-4df4-ad88-248e8a757e05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zeroth, human screen name collection part 1"
      ],
      "metadata": {
        "id": "vK03daQVUfV_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "006iG3e-6YVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, human screen name collection part 2"
      ],
      "metadata": {
        "id": "F018XIyt8XIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "screen_names = {\"contains_dataset\":[]}\n",
        "for filename in part_two_files:\n",
        "  tsvname = '/content/drive/MyDrive/botsdata/'+filename+'.tsv'\n",
        "  jsonname = '/content/drive/MyDrive/botsdata/'+filename+'_tweets.json'\n",
        "\n",
        "  bot_or_human_df = pd.read_csv(tsvname, sep='\\t',names = ['id','type'])\n",
        "  f = open(jsonname)\n",
        "  data = json.load(f)\n",
        "\n",
        "  collecting = False\n",
        "  if (filename not in screen_names[\"contains_dataset\"]) and (filename in train_data):\n",
        "    collecting = True\n",
        "    screen_names[\"contains_dataset\"].append(filename)\n",
        "\n",
        "  for line in data:\n",
        "    screen_name = line['user']['screen_name']\n",
        "    user = int(line['user']['id_str'])\n",
        "    temp_df = bot_or_human_df.loc[bot_or_human_df['id'] == user, 'type']\n",
        "    if temp_df.size >= 1:\n",
        "      if (temp_df.iloc[0] == 'human') and collecting:\n",
        "          if screen_name not in screen_names.keys():\n",
        "            screen_names[screen_name] = 1\n",
        "          else:\n",
        "            screen_names[screen_name] += 1"
      ],
      "metadata": {
        "id": "OC5oKJGO06AB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Second, screen name bigram probability calculations"
      ],
      "metadata": {
        "id": "HXSXfoTa9llJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#credit to ....\n",
        "def get_char_ngrams(string, n):\n",
        "    \"\"\"\n",
        "    This function takes a string and a positive integer n as input\n",
        "\n",
        "    It returns all character n-grams from the string of size n\n",
        "    They are stored in a list\n",
        "    \"\"\"\n",
        "    char_ngrams = []\n",
        "    for i in range(len(string) - n + 1):\n",
        "        char_ngrams.append(string[i:i+n])\n",
        "    return char_ngrams\n",
        "\n",
        "def _normalize_helper(dictionary):\n",
        "    \"\"\"\n",
        "    Normalize a single dictionary\n",
        "    \"\"\"\n",
        "    new_dict = {}\n",
        "    total = sum(dictionary.values())\n",
        "    for k, v in dictionary.items():\n",
        "        new_dict[k] = v / total\n",
        "    return new_dict\n",
        "\n",
        "def normalize(dictionary):\n",
        "    \"\"\"\n",
        "    This function takes dictionaries in two formats as input:\n",
        "    1. A dictionary where the keys are strings and the values are numbers\n",
        "                        OR\n",
        "    2. A dictionary of dictionaries. The keys of the inner dictionaries should be strings, and the values should be numbers.\n",
        "\n",
        "    It returns a new dictionary that is normalized such that the values add up to 1, representing a probability distribution. If the input is a dictionary of dictionaries, it normalizes all of the sub-dictionaries.\n",
        "    \"\"\"\n",
        "    if type(list(dictionary.values())[0]) == int:\n",
        "        return _normalize_helper(dictionary)\n",
        "\n",
        "    new_dict = {}\n",
        "    for k, v_dict in dictionary.items():\n",
        "        new_dict[k] = _normalize_helper(v_dict)\n",
        "    return new_dict\n",
        "\n",
        "chars = []\n",
        "for c in range(ord('A'), ord('Z')+1):\n",
        "  chars.append(chr(c))\n",
        "for c in range(ord('a'), ord('z')+1):\n",
        "  chars.append(chr(c))\n",
        "for c in range(0,10):\n",
        "  chars.append(str(c))\n",
        "chars.append('_')\n",
        "\n",
        "unigram = {}\n",
        "conditional = {}\n",
        "for name in all_names:\n",
        "  bigrams = get_char_ngrams(name, 2)\n",
        "  for bigram in bigrams:\n",
        "    if bigram[0] in unigram.keys():\n",
        "      unigram[bigram[0]] += 1\n",
        "    else:\n",
        "      unigram[bigram[0]] = 1\n",
        "\n",
        "    if bigram[0] in conditional.keys():\n",
        "      if bigram[1] in conditional[bigram[0]].keys():\n",
        "        conditional[bigram[0]][bigram[1]] += 1\n",
        "      else:\n",
        "        conditional[bigram[0]][bigram[1]] = 1\n",
        "    else:\n",
        "      conditional[bigram[0]] = {}\n",
        "      conditional[bigram[0]][bigram[1]] = 1\n",
        "\n",
        "\n",
        "# add 1 smoothing\n",
        "\n",
        "# for conditional\n",
        "for c1 in chars:\n",
        "  if c1 in conditional.keys():\n",
        "    for c2 in chars:\n",
        "      if c2 in conditional[c1].keys():\n",
        "        conditional[c1][c2] += 1\n",
        "      else:\n",
        "        conditional[c1][c2] = 1\n",
        "  else:\n",
        "    conditional[c1] = {}\n",
        "    for c2 in chars:\n",
        "      conditional[c1][c2] = 1\n",
        "\n",
        "# for unigram\n",
        "for c in chars:\n",
        "  if c in unigram.keys():\n",
        "    unigram[c] += 2*len(chars)\n",
        "  else:\n",
        "    unigram[c] = 2*len(chars)\n",
        "\n",
        "\n",
        "conditional = normalize(conditional)\n",
        "unigram = normalize(unigram)\n",
        "\n",
        "\n",
        "bigram_probs = {}\n",
        "for c1 in chars:\n",
        "  bigram_probs[c1] = {}\n",
        "  for c2 in chars:\n",
        "    bigram_probs[c1][c2] = conditional[c1][c2]*unigram[c1]\n",
        "\n",
        "\n",
        "def likelihood(screen_name):\n",
        "  likelihood = 1\n",
        "  bigram_name = get_char_ngrams(screen_name, 2)\n",
        "  for bigram in bigram_name:\n",
        "    likelihood *= bigram_probs[bigram[0]][bigram[1]]\n",
        "  likelihood = pow(likelihood,1/len(bigram_name))\n",
        "  print(likelihood)"
      ],
      "metadata": {
        "id": "fD5HAkLc9r9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Third, part_one_files data preprocessing"
      ],
      "metadata": {
        "id": "4kf565IZ8q3N"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n_WQ_WbWPCM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fourth, part_two_files data preprocessing"
      ],
      "metadata": {
        "id": "UX3O8wl48y31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for filename in part_two_files:\n",
        "  tsvname = '/content/drive/MyDrive/botsdata/'+filename+'.tsv'\n",
        "  jsonname = '/content/drive/MyDrive/botsdata/'+filename+'_tweets.json'\n",
        "\n",
        "  bot_or_human_df = pd.read_csv(tsvname, sep='\\t',names = ['id','type'])\n",
        "  f = open(jsonname)\n",
        "  data = json.load(f)\n",
        "\n",
        "  arr = []\n",
        "  for line in data:\n",
        "    row = []\n",
        "    user = int(line['user']['id_str'])\n",
        "    # row.append(line['user']['id_str'])\n",
        "    row.append(line['user']['statuses_count'])\n",
        "    row.append(line['user']['followers_count'])\n",
        "    row.append(line['user']['friends_count'])\n",
        "    row.append(line['user']['favourites_count'])\n",
        "    row.append(line['user']['listed_count'])\n",
        "    row.append(line['user']['default_profile'])\n",
        "    row.append(line['user']['profile_use_background_image'])\n",
        "    row.append(line['user']['verified'])\n",
        "\n",
        "\n",
        "    dt_born = parse(line['user']['created_at'])\n",
        "    dt_curr = parse(line['created_at'])\n",
        "    age = dt_curr - dt_born\n",
        "    user_age = age.days*24 + age.seconds/3600\n",
        "\n",
        "    row.append(line['user']['statuses_count'] / user_age)\n",
        "    row.append(line['user']['followers_count'] / user_age)\n",
        "    row.append(line['user']['friends_count'] / user_age)\n",
        "    row.append(line['user']['favourites_count'] / user_age)\n",
        "    row.append(line['user']['listed_count'] / user_age)\n",
        "    row.append(line['user']['followers_count'] / max(1,line['user']['friends_count']))\n",
        "\n",
        "    screen_name = line['user']['screen_name']\n",
        "    name = line['user']['name']\n",
        "\n",
        "    row.append(len(screen_name))\n",
        "    row.append(sum(c.isdigit() for c in screen_name))\n",
        "    row.append(len(name))\n",
        "    row.append(sum(c.isdigit() for c in name))\n",
        "    row.append(len(line['user']['description']))\n",
        "    row.append(str(likelihood(screen_name)))\n",
        "\n",
        "\n",
        "\n",
        "    df2 = df.loc[df['id'] == user, 'type']\n",
        "    if df2.size >= 1:\n",
        "      if (df2.iloc[0] == 'human'):\n",
        "        row.append('0')\n",
        "        arr.append(row)\n",
        "        \n",
        "      elif (df2.iloc[0] == 'bot'):\n",
        "        row.append('1')\n",
        "        arr.append(row)\n",
        "\n",
        "  f.close()\n",
        "  outname = filename + '.csv'\n",
        "\n",
        "\n",
        "  with open(outname, 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile, delimiter=',',\n",
        "                            quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
        "    writer.writerow(['statuses_count','followers_count','friends_count','favourites_count','listed_count','default_profile','profile_use_background_image','verified','tweet_freq','followers_growth_rate','friends_growth_rate','favourites_growth_rate','listed_growth_rate', 'followers_friends_ratio', 'screen_name_length','num_digits_in_screen_name', 'name_length','num_digits_in_name','description_length','screen_name_likelihood', 'is_bot'])\n",
        "    for row in arr:\n",
        "      writer.writerow(row)"
      ],
      "metadata": {
        "id": "PaC4NVd4507X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}